{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a59889-9391-41e8-9e15-f328e860ae37",
   "metadata": {},
   "source": [
    "# Date Conversion with Encoder-Decoder RNN in TensorFlow and PyTorch\n",
    "In this project, I build Encoder-Decoder RNN from scratch to convert date format: from - April 22, 2019 - to another format - 2019-04-22 in two frameworks: Tensorflow and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee249b4",
   "metadata": {},
   "source": [
    "* [Generate Dataset](#Generate-Datasetle)\n",
    "* [PyTorch Implementation](#PyTorch-Implementation)\n",
    "    * [Data Preparation](#Data-Preparation)\n",
    "    * [Build RNN](#build-rnn)\n",
    "    * [Train the model](#train-the-model)\n",
    "    * [Inference](#inference)\n",
    "* [TensorFlow Implementation](#tensorflow-implementation)\n",
    "    * [Build RNN in TF](#build-rnn-in-tf)\n",
    "    * [Train the model in TF](#train-the-model-in-tf)\n",
    "    * [Inference in TF](#inference-in-tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1b5631-f825-455d-b81c-a6d800d638ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a269854-745a-401f-be01-2ad096152222",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da59f60-5577-4aad-854a-d46495e52f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(start, end, time_format, size):\n",
    "\n",
    "    stime = time.mktime(time.strptime(start, time_format))\n",
    "    etime = time.mktime(time.strptime(end, time_format))\n",
    "    \n",
    "    random_dates = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        prop = random.random()\n",
    "        rtime = stime + prop * (etime - stime)\n",
    "        rtime = time.strftime(time_format, time.localtime(rtime))\n",
    "        random_dates.append(rtime)\n",
    "\n",
    "    return random_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6a15e0-0d4e-4c8a-983f-a5669c22e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10000\n",
    "y_train = generate_dataset('1900-01-01', '2024-12-31', '%Y-%m-%d', train_size)\n",
    "X_train = [datetime.strptime(x, '%Y-%m-%d').strftime(\"%B %d, %Y\") for x in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7d7a6e-b72a-4ff5-8731-8e8181d06315",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 5\n",
    "y_test = generate_dataset('1900-01-01', '2024-12-31', '%Y-%m-%d', test_size)\n",
    "X_test = [datetime.strptime(x, '%Y-%m-%d').strftime(\"%B %d, %Y\") for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42cea25d-331a-49d1-8e3b-d107cd8ceafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: February 01, 1900\n",
      "Target date: 1900-02-01\n"
     ]
    }
   ],
   "source": [
    "print(\"Source date:\", X_train[10])\n",
    "print(\"Target date:\",y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff61eb0-e0b6-4e7d-a258-f966ca5250b7",
   "metadata": {},
   "source": [
    "## PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33028a04-5611-44f3-a075-a698e82ceb6a",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Creat `DatesDataset` class that:\n",
    "- Tokenizes and maps date strings to indices for both source and target sequences.\n",
    "- Adds special tokens for sequence start, end, and unknown values.\n",
    "- Converts each date string into a PyTorch tensor for training.\n",
    "- Computes vocab sizes and maximum sequence lengths for handling different date formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41845c0d-6393-4c91-bafe-52c47ad8d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset for date conversion tasks. This dataset prepares input\n",
    "    and target tensors by tokenizing and encoding date strings.\n",
    "\n",
    "    Attributes:\n",
    "        source (list): List of input date strings.\n",
    "        target (list): List of target date strings.\n",
    "        input_vocab (set): Set of unique tokens in the input dates.\n",
    "        output_vocab (set): Set of unique tokens in the target dates.\n",
    "        input_tensor (Tensor): Encoded tensor of input date sequences.\n",
    "        output_tensor (Tensor): Encoded tensor of target date sequences.\n",
    "        input_vocab_dim (int): Size of the input vocabulary.\n",
    "        output_vocab_dim (int): Size of the output vocabulary.\n",
    "        input_max_seq_length (int): Maximum sequence length for input dates.\n",
    "        output_max_seq_length (int): Maximum sequence length for target dates.\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.source = X_train\n",
    "        self.target = y_train\n",
    "        \n",
    "        self.input_vocab = set()\n",
    "        self.output_vocab =set()\n",
    "        \n",
    "        self.input_tensor = self.prepare_source_data(self.source)\n",
    "        self.output_tensor = self.prepare_target_data(self.target)\n",
    "\n",
    "        self.input_vocab_dim = len(self.human_char_idx)\n",
    "        self.output_vocab_dim = len(self.machine_char_idx)\n",
    "        \n",
    "        self.input_max_seq_length = max(len(txt) for txt in self.source)\n",
    "        self.output_max_seq_length = max(len(txt) for txt in self.target)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_date = self.input_tensor[idx]\n",
    "        target_date = self.output_tensor[idx]\n",
    "        \n",
    "        return source_date, target_date\n",
    "    \n",
    "    def prepare_source_data(self, data):\n",
    "        \"\"\"\n",
    "        Tokenizes and encodes input date sequences.\n",
    "        \n",
    "        Args:\n",
    "            data (list): List of input date strings.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tensor of encoded input sequences.\n",
    "        \"\"\"\n",
    "        data_token = [x.replace(',', ' ,').split() for x in data]\n",
    "        \n",
    "        for token in data_token:\n",
    "            self.input_vocab.update(token)\n",
    "\n",
    "        special_tokens = ['<START>', '<END>', '<UNK>']\n",
    "        self.input_vocab.update(special_tokens)\n",
    "     \n",
    "        self.human_char_idx = {token: i for i, token in enumerate(self.input_vocab)}\n",
    "        self.human_idx_char = {i: token for i, token in enumerate(self.input_vocab)}\n",
    "        \n",
    "        indices = []\n",
    "        for tokens in data_token:\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "            index = [self.human_char_idx.get(token, self.human_char_idx['<UNK>']) for token in tokens]\n",
    "            indices.append(index)\n",
    "        tensors = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        return tensors\n",
    "    \n",
    "    def prepare_target_data(self, data):\n",
    "        \"\"\"\n",
    "        Tokenizes and encodes target date sequences.\n",
    "        \n",
    "        Args:\n",
    "            data (list): List of target date strings.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Tensor of encoded target sequences.\n",
    "        \"\"\"\n",
    "        data_token = [y.replace('-', ' - ').split() for y in data]\n",
    "        \n",
    "        for token in data_token:\n",
    "            self.output_vocab.update(token)\n",
    "\n",
    "        special_tokens = ['<START>', '<END>', '<UNK>']\n",
    "        self.output_vocab.update(special_tokens)\n",
    "\n",
    "        self.machine_char_idx = {token: i for i, token in enumerate(self.output_vocab)}\n",
    "        self.machine_idx_char = {i: token for i, token in enumerate(self.output_vocab)}\n",
    "\n",
    "        indices = []\n",
    "        for tokens in data_token:\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "            index = [self.machine_char_idx.get(token, self.machine_char_idx['<UNK>']) for token in tokens]\n",
    "            indices.append(index)\n",
    "        tensors = torch.tensor(indices, dtype=torch.long)\n",
    "        \n",
    "        return tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93801e2a-75c6-4df0-ba68-57c49f8182d8",
   "metadata": {},
   "source": [
    "Load dataset with dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da31a8b-1484-49ce-84d2-44b2724788d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatesDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12985b0e-777f-4086-8d8e-b78e96549e29",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "The RNN model is built with an encoder-decoder architecture, where both the encoder and decoder are constructed using Long Short-Term Memory (LSTM) layers. The encoder processes input sequences and captures context, while the decoder generates output sequences based on this encoded information.\n",
    "\n",
    "The model structure:\n",
    "**Encoder**\n",
    "- Receives an input sequence of token IDs (encoder_inputs), which are first embedded using an `Embedding` layer with an emdedding dimension, and then passed through dropout for regularization.\n",
    "- The embedded sequence is processed by the LSTM, which outputs `hidden` and `cell` states. These states capture the context of the input sequence and are used as input for the decoder\n",
    "\n",
    "**Decoder**\n",
    "- The input token is unsqueezed then embedded to produce a dense vector.\n",
    "- The embedded input is passed through the LSTM along with the `hidden` and `cell` states from the encoder. The LSTM output provides updated hidden and cell states.\n",
    "- The LSTM’s output at each time step is passed through the linaer layer to produce a probabilities over the output vocabulary, allowing the decoder to predict the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "575a49de-7a20-4611-bc96-e75be5e9c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f845cbf8-e336-4fee-b9a7-aaaebef0841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c9b10-1ebc-425c-b733-0bc81b130ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        assert encoder.hidden_size == decoder.hidden_size, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        trg_length = target.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_length, trg_vocab_size).to(source.device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "    \n",
    "        input = target[:, 0]\n",
    "\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if teacher_force else top1\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eeae90-b99a-4917-a7aa-302faa06c3b3",
   "metadata": {},
   "source": [
    "Connect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61c5ffbc-8cab-4083-9d1a-e86f09200d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(dataset.human_char_idx)\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "embedding_size = 256\n",
    "dropout_p = 0.5\n",
    "encoder = Encoder(\n",
    "    input_size=input_size,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    p=dropout_p)\n",
    "\n",
    "decoder = Decoder(input_size=input_size,\n",
    "                  embedding_size=embedding_size,\n",
    "                  hidden_size=hidden_size,\n",
    "                  output_size=input_size,\n",
    "                  num_layers=num_layers\n",
    "                 )\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3392ea66-d7b7-4963-a0bc-12d7b4213bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(172, 256)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(172, 256)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "    (fc): Linear(in_features=256, out_features=172, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc7e74-d32d-47db-b98c-0c9b322e1457",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c59fa672-82e7-4e9b-81f3-60e292e58aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch} ----------\")\n",
    "        epoch_loss = 0\n",
    "        for source, target in dataloader:\n",
    "            source, target = source.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(source, target)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            target = target[:, 1:].reshape(-1)\n",
    "            \n",
    "            assert target.max() < output.size(-1), f\"Target value exceeds number of classes: max {target.max()} >= {output.size(-1)}\"\n",
    "            assert target.min() >= 0, f\"Target value contains negative indices: min {target.min()} < 0\"\n",
    "\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf42f12-1ae6-4e38-8ae7-41e09ebd4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb6673-37cb-4598-add3-57d0c597bc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ----------\n",
      "Epoch 1/10, Loss: 2.2459\n",
      "Epoch 1 ----------\n",
      "Epoch 2/10, Loss: 0.6260\n",
      "Epoch 2 ----------\n",
      "Epoch 3/10, Loss: 0.1328\n",
      "Epoch 3 ----------\n",
      "Epoch 4/10, Loss: 0.0363\n",
      "Epoch 4 ----------\n",
      "Epoch 5/10, Loss: 0.0171\n",
      "Epoch 5 ----------\n",
      "Epoch 6/10, Loss: 0.0102\n",
      "Epoch 6 ----------\n",
      "Epoch 7/10, Loss: 0.0070\n",
      "Epoch 7 ----------\n",
      "Epoch 8/10, Loss: 0.0051\n",
      "Epoch 8 ----------\n",
      "Epoch 9/10, Loss: 0.0038\n",
      "Epoch 9 ----------\n",
      "Epoch 10/10, Loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "train(model, dataloader, optimizer, criterion, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f646a-e1b6-4296-97fb-525286239544",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5a81d7c0-23b9-4cac-a8c3-4bb5da48241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        source = dataset.prepare_source_data([x]).to(model.encoder.embedding.weight.device)\n",
    "        hidden, cell = model.encoder(source)\n",
    "    \n",
    "        target = torch.tensor([[dataset.machine_char_idx['<START>']]], device=source.device)\n",
    "        output_str = ''\n",
    "        \n",
    "        for _ in range(dataset.output_max_seq_length):  # max length of output \n",
    "            output, hidden, cell = model.decoder(target.squeeze(1), hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == dataset.machine_char_idx['<END>']:\n",
    "                break\n",
    "            output_str += dataset.machine_idx_char[top1.item()]\n",
    "            target = top1.unsqueeze(0)\n",
    "        \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "201b1a62-aff8-4d6d-8c8a-3fa5ac74a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "source date: September 22, 1956\n",
      "target date: 1956-09-22\n",
      "prediction: 1956-09-22\n",
      "\n",
      "test 1\n",
      "source date: June 11, 2012\n",
      "target date: 2012-06-11\n",
      "prediction: 2012-06-11\n",
      "\n",
      "test 2\n",
      "source date: July 02, 1988\n",
      "target date: 1988-07-02\n",
      "prediction: 1988-07-02\n",
      "\n",
      "test 3\n",
      "source date: October 08, 1917\n",
      "target date: 1917-10-08\n",
      "prediction: 1917-10-08\n",
      "\n",
      "test 4\n",
      "source date: August 17, 1934\n",
      "target date: 1934-08-17\n",
      "prediction: 1934-08-17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    pred =test(model, X_test[i], dataset)\n",
    "    print(f\"test {i}\")\n",
    "    print(f\"source date: {X_test[i]}\")\n",
    "    print(f\"target date: {y_test[i]}\")\n",
    "    print(f\"prediction: {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d427d5-806a-4bbb-b7e8-3582933a28f5",
   "metadata": {},
   "source": [
    "## TensorFlow Implementation\n",
    "### Build RNN in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f7f63b-0041-475f-805f-dbad297c0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e272697-e792-4efa-912d-2a0ffb7981ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models(num_encoder_tokens, num_decoder_tokens, embed_size, latent_dim):\n",
    "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    encoder_embeddings = keras.layers.Embedding(num_encoder_tokens, embed_size)(encoder_inputs)\n",
    "    encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    decoder_embeddings = keras.layers.Embedding(num_decoder_tokens, embed_size)(decoder_inputs)\n",
    "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
    "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982a7cd9-b730-4cde-9c5b-f7b74a1e480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_models(num_encoder_tokens=dataset.input_vocab_dim,\n",
    "                     num_decoder_tokens=dataset.output_vocab_dim,\n",
    "                     embed_size=50,\n",
    "                     latent_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e47c440-5316-4ccf-8d0f-b8515671ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,600</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">314,368</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">314,368</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,120</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)  │      \u001b[38;5;34m8,600\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)  │      \u001b[38;5;34m8,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m314,368\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m314,368\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m) │     \u001b[38;5;34m41,120\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">686,456</span> (2.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m686,456\u001b[0m (2.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">686,456</span> (2.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m686,456\u001b[0m (2.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051e603",
   "metadata": {},
   "source": [
    "### Train the model in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6069a1bf-0592-4e75-b923-33c51a9a1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1510020b-5279-4a3d-9f49-0192774d30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = dataset.input_tensor\n",
    "decoder_input_data = dataset.output_tensor[:, :-1]\n",
    "decoder_target_data = dataset.output_tensor[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "213cd45a-e627-4b50-9ffb-a8e9b44df24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.7925 - loss: 0.9364 - val_accuracy: 0.8330 - val_loss: 0.8207\n",
      "Epoch 2/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.8441 - loss: 0.7498 - val_accuracy: 0.9057 - val_loss: 0.4565\n",
      "Epoch 3/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9494 - loss: 0.3211 - val_accuracy: 0.9986 - val_loss: 0.0798\n",
      "Epoch 4/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9950 - loss: 0.0666 - val_accuracy: 0.9999 - val_loss: 0.0236\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0166 - val_accuracy: 1.0000 - val_loss: 0.0104\n",
      "Epoch 6/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 0.0066\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 8/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
      "Epoch 10/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2927b6bd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415a8e32",
   "metadata": {},
   "source": [
    "### Inference in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84516094-591a-4218-878e-952631ac177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(model, x, dataset):\n",
    "    encoder_input = dataset.prepare_source_data([x])\n",
    "    encoder_input = tf.expand_dims(encoder_input, -1)\n",
    "    decoder_input = tf.constant([[dataset.machine_char_idx['<START>']]])\n",
    "\n",
    "    target_seq = []\n",
    "    \n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        predictions = model([encoder_input, decoder_input])\n",
    "        predicted_idx = tf.argmax(predictions[0, -1, :], axis=-1).numpy()\n",
    "        if predicted_idx == dataset.machine_char_idx['<END>']:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            target_seq.append(predicted_idx)\n",
    "            decoder_input = tf.concat([decoder_input, [[predicted_idx]]], axis=-1)\n",
    "            \n",
    "    target = ''.join([dataset.machine_idx_char.get(id, '<UNK>') for id in target_seq \n",
    "                      if id not in [dataset.machine_char_idx['<START>'], dataset.machine_char_idx['<END>']]])\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d0d50d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "source date: April 26, 1950\n",
      "target date: 1950-04-26\n",
      "prediction: 1950-04-26\n",
      "\n",
      "test 1\n",
      "source date: December 10, 1959\n",
      "target date: 1959-12-10\n",
      "prediction: 1959-12-10\n",
      "\n",
      "test 2\n",
      "source date: January 20, 2017\n",
      "target date: 2017-01-20\n",
      "prediction: 2017-01-20\n",
      "\n",
      "test 3\n",
      "source date: March 07, 1961\n",
      "target date: 1961-03-07\n",
      "prediction: 1961-03-07\n",
      "\n",
      "test 4\n",
      "source date: January 08, 1917\n",
      "target date: 1917-01-08\n",
      "prediction: 1917-01-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    \n",
    "    output = decode_sequence(model, X_test[i], dataset)\n",
    "    print(f\"test {i}\")\n",
    "    print(f\"source date: {X_test[i]}\")\n",
    "    print(f\"target date: {y_test[i]}\")\n",
    "    print(\"prediction:\", output)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
